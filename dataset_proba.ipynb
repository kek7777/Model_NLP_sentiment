{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chat_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# nltk.download('all')                                   # for  working with NLTL function, after the first start should pick #nltk.download('all') \u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchat_words\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chat_word                       \u001b[38;5;66;03m# for translate slang of charts to text\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mautocorrect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Speller                        \u001b[38;5;66;03m# for Spelling Correction\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter, OrderedDict                        \u001b[38;5;66;03m# for definition of unique words (tokens) in dataframe\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chat_words'"
     ]
    }
   ],
   "source": [
    "\"\"\" This notebook used for preporation data to NLP modeling.\n",
    " Proceses of Tokenization, Stemming, Lemmatization, Handling text (Remove HTML Tag, URLs, Emojies and other) are here.   \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re                                              # Import Regular Expression (remove HTML tags)\n",
    "import string                                          # Import Punctuation \n",
    "from textblob import TextBlob                          # Import this Library to Handle the Spelling Issue\n",
    "import nltk\n",
    "from nltk.corpus import stopwords                      #  NLTK library to remove Stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji                                           # for translating symbol to text\n",
    "import spacy                                           # for tokenization\n",
    "import spacy.cli\n",
    "# spacy.cli.download(\"en_core_web_sm\")                 # for  working with spacy, after the first start should pick  # spacy.cli.download(\"en_core_web_lg\")\n",
    "from nltk.stem.porter import PorterStemmer             # for stemming\n",
    "# nltk.download('all')                                   # for  working with NLTL function, after the first start should pick #nltk.download('all') \n",
    "from sklearn.model_selection import train_test_split\n",
    "from chat_words import chat_word                       # for translate slang of charts to text\n",
    "from autocorrect import Speller                        # for Spelling Correction\n",
    "from collections import Counter, OrderedDict                        # for definition of unique words (tokens) in dataframe\n",
    "from torchtext.vocab import vocab  \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Admin\\WORK\\Project_CV\\Model_NLP_sentiment\\data\\IMDB Dataset.csv')    # insert path to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)                                                # check dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transform sentiment  into the number labels\n",
    "# def transform_label(label):\n",
    "#     return 1 if label == 'positive' else 0\n",
    "\n",
    "# df['label'] = df['sentiment'].apply(transform_label)\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Access the corpus and target variables\n",
    "# x = df.review\n",
    "# y = df.label                                                                            \n",
    "\n",
    "# # train test splitting\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.0005, random_state=0)\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose items for preprocessing: True or False\n",
    "\n",
    "lower = True                           # LoweCasing Text\n",
    "remove_html = True                     # Remove HTML Tag\n",
    "remove_url = True                       # Remove URLs\n",
    "remove_punc = True                     # Remove punctuation\n",
    "change_chat = True                     # Handling chat's words to words\n",
    "spell_cor = True                       # Spelling Correction\n",
    "remove_stopword = True                 # Remove StopWords\n",
    "remove_emoji = True                  # Handling Emojies to words\n",
    "use_stemm = False                       # Apply Stemming\n",
    "use_lemm = True                        # Apply Lemmatization\n",
    "use_token = True                        # Apply Tokenization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing\n",
    "def preprocessing (text):\n",
    "        \n",
    "    if lower:                                                        # LoweCasing Text\n",
    "        text = text.lower()\n",
    "                                            \n",
    "    if remove_html:\n",
    "        pattern_1 = re.compile('<.*?>')                              # constant using one regular expression\n",
    "        text = re.sub(pattern_1, r'', text)                          # Remove HTML Tags (changes ('<.*?>') to gap \" \")\n",
    "\n",
    "    if remove_url:\n",
    "        pattern_2 = re.compile(r'https?://\\S+|www\\.\\S+')             #  Remove URLs from Text or Whole Corpus.\n",
    "        text = pattern_2.sub(r'', text)\n",
    "\n",
    "    if remove_punc:\n",
    "        punc = string.punctuation                                    # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', punc))\n",
    "\n",
    "    if change_chat:\n",
    "        new_text = []                                                 # changes chat's words to text       \n",
    "        for i in text.split():\n",
    "            if i.upper() in chat_word:\n",
    "                new_text.append(chat_word[i.upper()])\n",
    "            else:\n",
    "                new_text.append(i)\n",
    "        text = \" \".join(new_text)\n",
    "        new_text.clear()\n",
    "\n",
    "    if spell_cor:\n",
    "        spell = Speller(lang='en')                                    # Spelling Correction\n",
    "        text = spell(text)\n",
    "\n",
    "    if remove_stopword:\n",
    "        stopword = stopwords.words('english')                          # Handling StopWords\n",
    "        for word in text.split():\n",
    "            if word in stopword:\n",
    "                new_text.append('')\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        pattern_3 = new_text[:]\n",
    "        text = \" \".join(pattern_3)\n",
    "\n",
    "    if remove_emoji:\n",
    "        text = emoji.demojize(text)                                   # Handling Emojies \n",
    "\n",
    "    \n",
    "    if use_stemm:\n",
    "        stemmer = PorterStemmer()                                     # Stemming\n",
    "        text = \" \".join([stemmer.stem(word)\n",
    "                  for word in text.split()])\n",
    "                            \n",
    "        \n",
    "    if use_lemm:\n",
    "        lemmatizer = WordNetLemmatizer()                              #Lemmatization\n",
    "        words = nltk.word_tokenize(text)\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        text = ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "    if use_token:\n",
    "        nlp = spacy.load('en_core_web_sm')                            # the English language model 'en_core_web_sm'\n",
    "        text = nlp(text)                                              \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Applying function for preprocessing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# # X_trainn = X_train.apply(preprocessing)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# x_testt = x_test.apply(preprocessing)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# x_testt.head(2)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocessing)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# # Applying function for preprocessing\n",
    "\n",
    "# # X_trainn = X_train.apply(preprocessing)\n",
    "# x_testt = x_test.apply(preprocessing)\n",
    "# x_testt.head(2)\n",
    "df['review'] = df['review'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @lapcat need to send 'em to my accountant tomo...\n",
       "1    <html><body><p> Movie 1</p><p> Actor - Aamir K...\n",
       "2    Check out my notebook https://www.kaggle.com/c...\n",
       "3                                  IMHO he is the best\n",
       "4             FYI Islamabad is the capital of Pakistan\n",
       "5    ceertain conditionas duriing seveal ggeneratio...\n",
       "6    probably my all-time favorite movie, a story o...\n",
       "7                            Loved the movie. It was 😘\n",
       "8                            walk walks walking walked\n",
       "9    He was running and eating at same time. He has...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This block for checking  def Proprocessing\n",
    "\n",
    "check_data = {\n",
    "    \"review\": [\"@lapcat need to send 'em to my accountant tomorrow. oddly, i wasn't even referring to my taxes. those are supporting evidence, though. \",\n",
    "                \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\",\n",
    "                 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1', 'IMHO he is the best', 'FYI Islamabad is the capital of Pakistan',\n",
    "                 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner', 'probably my all-time favorite movie, a story of selflessness,'\n",
    "                 ' sacrifice and dedication to a noble cause', \"Loved the movie. It was 😘\", \"walk walks walking walked\",\n",
    "                 \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "]\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_check = pd.DataFrame(check_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "prov = df_check['review']\n",
    "prov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (lancet, need, send, em, accountant, tomorrow,...\n",
       "1       (movie, 1, actor, amir, khan, click, download)\n",
       "2                                    (check, notebook)\n",
       "3           (In, My, Honest, /, Humble, Opinion, best)\n",
       "4    (For, Your, Information, islamabad, capital, p...\n",
       "5    (certain, condition, several, generation, modi...\n",
       "6    (probably, alltime, favorite, movie, story, se...\n",
       "7            (loved, movie, :, face_blowing_a_kiss, :)\n",
       "8                        (walk, walk, walking, walked)\n",
       "9    (running, eating, Tears, eye, bad, habit, swim...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prov_check = prov.apply(preprocessing)\n",
    "prov_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all processed r\n",
    "# Definition of unique words (tokens) in dataframe\n",
    "\n",
    "token_counts = Counter()\n",
    "for word in x_testt:\n",
    "    token_counts.update(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 2520\n"
     ]
    }
   ],
   "source": [
    "print('Dictionary size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sorted_by_freg_tuples = sorted (token_counts.items(), key=lambda x: x[1], reverse = True)\n",
    "ordered_dict = OrderedDict(sorted_by_freg_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)\n",
    "# print([vocab[token] for token in ['Hot','run'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_testt))\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'review'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\WORK\\Project_CV\\Model_NLP_sentiment\\NLP_sent_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:175\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'review'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_text2 \u001b[38;5;241m=\u001b[39m \u001b[43mx_testt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# get all processed reviews\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# merge into single variable, separated by whitespaces\u001b[39;00m\n\u001b[0;32m      6\u001b[0m all_text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(all_text2)     \n",
      "File \u001b[1;32mc:\\Users\\Admin\\WORK\\Project_CV\\Model_NLP_sentiment\\NLP_sent_venv\\lib\\site-packages\\pandas\\core\\series.py:1130\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\WORK\\Project_CV\\Model_NLP_sentiment\\NLP_sent_venv\\lib\\site-packages\\pandas\\core\\series.py:1246\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1246\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\WORK\\Project_CV\\Model_NLP_sentiment\\NLP_sent_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'review'"
     ]
    }
   ],
   "source": [
    "all_text2 = x_testt['review'].tolist()\n",
    "\n",
    "# get all processed reviews\n",
    "\n",
    "# merge into single variable, separated by whitespaces\n",
    "all_text2 = ' '.join(all_text2)     \n",
    "# obtain list of words\n",
    "words = all_text2.split()\n",
    "\n",
    "# check our list\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 20000\n",
    "TEXT = torchtext.legacy.data.Field(tokenize = 'spacy', tokenizer_language = 'en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_sent_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
