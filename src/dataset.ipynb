{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\WORK\\Project_CV\\Model_NLP_sentiment\\NLP_sent_venv\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\Admin\\WORK\\Project_CV\\Model_NLP_sentiment\\NLP_sent_venv\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This notebook used for preporation data to NLP modeling.\n",
    " Proceses of Tokenization, Stemming, Lemmatization, Handling text (Remove HTML Tag, URLs, Emojies and other) are here.   \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re                                              # Import Regular Expression (remove HTML tags)\n",
    "import string                                          # Import Punctuation \n",
    "from textblob import TextBlob                          # Import this Library to Handle the Spelling Issue\n",
    "import nltk\n",
    "from nltk.corpus import stopwords                      #  NLTK library to remove Stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji                                           # for translating symbol to text\n",
    "import spacy                                           # for tokenization\n",
    "import spacy.cli\n",
    "# spacy.cli.download(\"en_core_web_sm\")                 # for  working with spacy, after the first start should pick  # spacy.cli.download(\"en_core_web_lg\")\n",
    "from nltk.stem.porter import PorterStemmer             # for stemming\n",
    "# nltk.download('all')                                   # for  working with NLTL function, after the first start should pick #nltk.download('all') \n",
    "from sklearn.model_selection import train_test_split\n",
    "from chat_words import chat_word                       # for translate slang of charts to text\n",
    "from autocorrect import Speller                        # for Spelling Correction\n",
    "from collections import Counter, OrderedDict           # for definition of unique words (tokens) in dataframe\n",
    "from torchtext.vocab import vocab  \n",
    "from tqdm import tqdm                                   # progressbar\n",
    "tqdm.pandas() \n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Admin\\WORK\\Project_CV\\Model_NLP_sentiment\\data\\IMDB Dataset.csv')    # insert path to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)                                                # check dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label\n",
       "0  One of the other reviewers has mentioned that ...  positive      1\n",
       "1  A wonderful little production. <br /><br />The...  positive      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform sentiment  into the number labels\n",
    "def transform_label(label):\n",
    "    return 1 if label == 'positive' else 0\n",
    "\n",
    "df['label'] = df['sentiment'].apply(transform_label)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose items for preprocessing: True or False\n",
    "\n",
    "lower = True                           # LoweCasing Text\n",
    "remove_html = True                     # Remove HTML Tag\n",
    "remove_url = True                       # Remove URLs\n",
    "remove_punc = True                     # Remove punctuation\n",
    "change_chat = True                     # Handling chat's words to words\n",
    "spell_cor = True                       # Spelling Correction\n",
    "remove_stopword = True                 # Remove StopWords\n",
    "remove_emoji = True                  # Handling Emojies to words\n",
    "use_stemm = False                       # Apply Stemming\n",
    "use_lemm = True                        # Apply Lemmatization\n",
    "use_token = False                       # Apply Tokenization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing\n",
    "def preprocessing (text):\n",
    "        \n",
    "    if lower:                                                        # LoweCasing Text\n",
    "        text = text.lower()\n",
    "                                            \n",
    "    if remove_html:\n",
    "        pattern_1 = re.compile('<.*?>')                              # constant using one regular expression\n",
    "        text = re.sub(pattern_1, r'', text)                          # Remove HTML Tags (changes ('<.*?>') to gap \" \")\n",
    "\n",
    "    if remove_url:\n",
    "        pattern_2 = re.compile(r'https?://\\S+|www\\.\\S+')             #  Remove URLs from Text or Whole Corpus.\n",
    "        text = pattern_2.sub(r'', text)\n",
    "\n",
    "    if remove_punc:\n",
    "        punc = string.punctuation                                    # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', punc))\n",
    "\n",
    "    if change_chat:\n",
    "        new_text = []                                                 # changes chat's words to text       \n",
    "        for i in text.split():\n",
    "            if i.upper() in chat_word:\n",
    "                new_text.append(chat_word[i.upper()])\n",
    "            else:\n",
    "                new_text.append(i)\n",
    "        text = \" \".join(new_text)\n",
    "        new_text.clear()\n",
    "\n",
    "    if spell_cor:\n",
    "        spell = Speller(lang='en')                                    # Spelling Correction\n",
    "        text = spell(text)\n",
    "\n",
    "    if remove_stopword:\n",
    "        stopword = stopwords.words('english')                          # Handling StopWords\n",
    "        for word in text.split():\n",
    "            if word in stopword:\n",
    "                new_text.append('')\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        pattern_3 = new_text[:]\n",
    "        text = \" \".join(pattern_3)\n",
    "\n",
    "    if remove_emoji:\n",
    "        text = emoji.demojize(text)                                   # Handling Emojies \n",
    "\n",
    "    \n",
    "    if use_stemm:\n",
    "        stemmer = PorterStemmer()                                     # Stemming\n",
    "        text = \" \".join([stemmer.stem(word)\n",
    "                  for word in text.split()])\n",
    "                            \n",
    "        \n",
    "    if use_lemm:\n",
    "        lemmatizer = WordNetLemmatizer()                              #Lemmatization\n",
    "        words = nltk.word_tokenize(text)\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        text = ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "    if use_token:\n",
    "        nlp = spacy.load('en_core_web_sm')                            # the English language model 'en_core_web_sm'\n",
    "        text = nlp(text)                                              # cmd:  python -m spacy download en_core_web_sm\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.loc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12204\\145210384.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new['clean'] = df_new['review'].apply(preprocessing)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>one reviewer mentioned watching 1 oz episode h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>thought wonderful way spend Tears eye hot summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>better matter love Tears eye money visually st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>probably alltime favorite movie story selfless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>sure would like see resurrection dated shunt s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>show amazing fresh innovative idea 70 first ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>encouraged positive comment film looking forwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>like original gut reaching laughter like movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>phil alien one quirky film humour based around...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment  label  \\\n",
       "0   One of the other reviewers has mentioned that ...  positive      1   \n",
       "1   A wonderful little production. <br /><br />The...  positive      1   \n",
       "2   I thought this was a wonderful way to spend ti...  positive      1   \n",
       "3   Basically there's a family where a little boy ...  negative      0   \n",
       "4   Petter Mattei's \"Love in the Time of Money\" is...  positive      1   \n",
       "5   Probably my all-time favorite movie, a story o...  positive      1   \n",
       "6   I sure would like to see a resurrection of a u...  positive      1   \n",
       "7   This show was an amazing, fresh & innovative i...  negative      0   \n",
       "8   Encouraged by the positive comments about this...  negative      0   \n",
       "9   If you like original gut wrenching laughter yo...  positive      1   \n",
       "10  Phil the Alien is one of those quirky films wh...  negative      0   \n",
       "\n",
       "                                                clean  \n",
       "0   one reviewer mentioned watching 1 oz episode h...  \n",
       "1   wonderful little production filming technique ...  \n",
       "2   thought wonderful way spend Tears eye hot summ...  \n",
       "3   basically there family little boy jake think t...  \n",
       "4   better matter love Tears eye money visually st...  \n",
       "5   probably alltime favorite movie story selfless...  \n",
       "6   sure would like see resurrection dated shunt s...  \n",
       "7   show amazing fresh innovative idea 70 first ai...  \n",
       "8   encouraged positive comment film looking forwa...  \n",
       "9   like original gut reaching laughter like movie...  \n",
       "10  phil alien one quirky film humour based around...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Applying function for preprocessing\n",
    "\n",
    "# # X_trainn = X_train.apply(preprocessing)\n",
    "# x_testt = x_test.apply(preprocessing)\n",
    "# x_testt.head(2)\n",
    "\n",
    "df_new['clean'] = df_new['review'].apply(preprocessing) \n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'reviewer',\n",
       " 'mentioned',\n",
       " 'watching',\n",
       " '1',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'hooked',\n",
       " 'right',\n",
       " 'exactly']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #get all processed reviews\n",
    "reviews = df_new.clean.values\n",
    "# merge into single variable, separated by whitespaces\n",
    "words = ' '.join(reviews)\n",
    "# obtain list of words\n",
    "words = words.split()\n",
    "\n",
    "# check our list\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "counter = Counter(words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "int2word = dict(enumerate(vocab, 1))\n",
    "int2word[0] = '<PAD>'\n",
    "word2int = {word: id for id, word in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'one', 2: 'film', 3: 'show', 4: 'movie', 5: 'would', 6: 'see', 7: 'first', 8: 'well', 9: 'like', 10: 'watching', 11: 'oz', 12: 'thing', 13: 'way', 14: 'eye', 15: 'violence', 16: 'go', 17: 'never', 18: 'say', 19: 'get', 20: 'truly', 21: 'comedy', 22: 'Tears', 23: 'character', 24: 'jake', 25: 'matter', 26: 'right', 27: 'city', 28: 'prison', 29: 'far', 30: 'forget', 31: 'pretty', 32: 'got', 33: 'may', 34: 'little', 35: 'great', 36: 'every', 37: 'air', 38: 'point', 39: 'still', 40: 'many', 41: 'u', 42: 'year', 43: 'ive', 44: 'parent', 45: 'make', 46: 'must', 47: 'drama', 48: 'better', 49: 'mr', 50: 'seems', 51: 'people', 52: 'different', 53: 'world', 54: 'find', 55: 'cast', 56: 'performance', 57: 'tv', 58: 'believe', 59: 'funny', 60: 'bad', 61: 'original', 62: 'episode', 63: 'exactly', 64: 'struck', 65: 'scene', 66: 'set', 67: 'word', 68: 'use', 69: 'state', 70: 'high', 71: 'home', 72: 'away', 73: 'appeal', 74: 'due', 75: 'wouldnt', 76: 'dare', 77: 'picture', 78: 'around', 79: 'nasty', 80: 'watched', 81: 'guard', 82: 'whole', 83: 'inmate', 84: 'kill', 85: 'become', 86: 'wonderful', 87: 'production', 88: 'technique', 89: 'give', 90: 'realism', 91: 'piece', 92: 'michael', 93: 'editing', 94: 'life', 95: 'really', 96: 'come', 97: 'rather', 98: 'play', 99: 'particularly', 100: 'thought', 101: 'even', 102: '2', 103: 'young', 104: 'interesting', 105: 'there', 106: 'zombie', 107: 'closet', 108: 'thriller', 109: 'similar', 110: '10', 111: 'love', 112: 'money', 113: 'watch', 114: 'human', 115: 'relation', 116: 'success', 117: 'new', 118: 'another', 119: 'next', 120: 'look', 121: 'live', 122: 'best', 123: 'acting', 124: 'good', 125: 'work', 126: 'probably', 127: 'boring', 128: 'old', 129: 'seen', 130: 'time', 131: 'kid', 132: 'shunt', 133: 'today', 134: 'sea', 135: 'let', 136: 'almost', 137: 'awful', 138: 'titel', 139: 'address', 140: 'reviewer', 141: 'mentioned', 142: '1', 143: 'hooked', 144: 'happened', 145: 'metre', 146: 'brutality', 147: 'unflinching', 148: 'trust', 149: 'faint', 150: 'hearted', 151: 'timid', 152: 'pull', 153: 'punch', 154: 'regard', 155: 'drug', 156: 'sex', 157: 'hardcore', 158: 'classic', 159: 'called', 160: 'nickname', 161: 'given', 162: 'onward', 163: 'maximum', 164: 'security', 165: 'penitentiary', 166: 'focus', 167: 'mainly', 168: 'emerald', 169: 'experimental', 170: 'section', 171: 'cell', 172: 'glass', 173: 'front', 174: 'face', 175: 'inwards', 176: 'privacy', 177: 'agenda', 178: 'em', 179: 'manyaryans', 180: 'muslim', 181: 'gangster', 182: 'latino', 183: 'christian', 184: 'italian', 185: 'irish', 186: 'moreno', 187: 'scuffle', 188: 'death', 189: 'dodgy', 190: 'dealing', 191: 'shady', 192: 'agreement', 193: 'main', 194: 'fact', 195: 'painted', 196: 'mainstream', 197: 'audience', 198: 'charm', 199: 'romance', 200: 'doesnt', 201: 'mess', 202: 'ever', 203: 'saw', 204: 'surreal', 205: 'couldnt', 206: 'ready', 207: 'developed', 208: 'taste', 209: 'accustomed', 210: 'level', 211: 'graphic', 212: 'injustice', 213: 'crooked', 214: 'sold', 215: 'nickel', 216: 'order', 217: 'mannered', 218: 'middle', 219: 'class', 220: 'turned', 221: 'pitch', 222: 'lack', 223: 'street', 224: 'skill', 225: 'experience', 226: 'comfortable', 227: 'uncomfortable', 228: 'viewingthats', 229: 'touch', 230: 'darker', 231: 'side', 232: 'filming', 233: 'assuming', 234: 'oldtimebbc', 235: 'fashion', 236: 'conforming', 237: 'sometimes', 238: 'discomforting', 239: 'sense', 240: 'entire', 241: 'actor', 242: 'extremely', 243: 'chosen', 244: 'sheen', 245: 'polar', 246: 'voice', 247: 'pat', 248: 'seamless', 249: 'guided', 250: 'reference', 251: 'williams', 252: 'diary', 253: 'entry', 254: 'worth', 255: 'terrific', 256: 'written', 257: 'performed', 258: 'masterful', 259: 'master', 260: 'fantasy', 261: 'traditional', 262: 'dream', 263: 'remains', 264: 'solid', 265: 'disappears', 266: 'knowledge', 267: 'sens', 268: 'concerning', 269: 'rton', 270: 'halliwell', 271: 'flat', 272: 'halliwells', 273: 'mural', 274: 'decorating', 275: 'surface', 276: 'terribly', 277: 'done', 278: 'spend', 279: 'hot', 280: 'summer', 281: 'weekend', 282: 'sitting', 283: 'conditioned', 284: 'theater', 285: 'lighthearted', 286: 'plot', 287: 'simplistic', 288: 'dialogue', 289: 'witty', 290: 'liable', 291: 'bread', 292: 'suspected', 293: 'serial', 294: 'killer', 295: 'disappointed', 296: 'realize', 297: 'match', 298: 'risk', 299: 'addiction', 300: 'proof', 301: 'woody', 302: 'allen', 303: 'fully', 304: 'control', 305: 'style', 306: 'grown', 307: 'lovethis', 308: 'id', 309: 'laughed', 310: 'wood', 311: 'decade', 312: 'impressed', 313: 'scarlet', 314: 'johnson', 315: 'managed', 316: 'tone', 317: 'sexy', 318: 'image', 319: 'jumped', 320: 'average', 321: 'spirited', 322: 'womanthis', 323: 'crown', 324: 'jewel', 325: 'career', 326: 'winter', 327: 'devil', 328: 'wear', 329: 'rada', 330: 'superman', 331: 'friend', 332: 'basically', 333: 'family', 334: 'boy', 335: 'think', 336: 'fighting', 337: 'timethis', 338: 'slower', 339: 'soap', 340: 'opera', 341: 'suddenly', 342: 'decides', 343: 'ramo', 344: 'youre', 345: 'going', 346: 'decide', 347: 'watchable', 348: 'divorcing', 349: 'arguing', 350: 'real', 351: 'totally', 352: 'ruin', 353: 'expected', 354: 'boogeyman', 355: 'instead', 356: 'meaningless', 357: 'spots3', 358: 'playing', 359: 'descent', 360: 'dialog', 361: 'shot', 362: 'ignore', 363: 'visually', 364: 'stunning', 365: 'offer', 366: 'vivid', 367: 'portrait', 368: 'telling', 369: 'power', 370: 'situation', 371: 'encounter', 372: 'variation', 373: 'arthur', 374: 'schnitzlers', 375: 'theme', 376: 'director', 377: 'transfer', 378: 'action', 379: 'present', 380: 'york', 381: 'meet', 382: 'connect', 383: 'connected', 384: 'person', 385: 'know', 386: 'previous', 387: 'contact', 388: 'stylish', 389: 'sophisticated', 390: 'luxurious', 391: 'taken', 392: 'habitatthe', 393: 'soul', 394: 'stage', 395: 'loneliness', 396: 'inhabits', 397: 'big', 398: 'place', 399: 'sincere', 400: 'fulfillment', 401: 'discern', 402: 'case', 403: 'encounterthe', 404: 'direction', 405: 'steve', 406: 'buscemi', 407: 'rosario', 408: 'dawson', 409: 'carol', 410: 'kane', 411: 'imperial', 412: 'adrian', 413: 'greater', 414: 'rest', 415: 'talented', 416: 'alive', 417: 'wish', 418: 'luck', 419: 'await', 420: 'anxious', 421: 'alltime', 422: 'favorite', 423: 'story', 424: 'selflessness', 425: 'sacrifice', 426: 'dedication', 427: 'noble', 428: 'cause', 429: 'preach', 430: 'despite', 431: '15', 432: 'last', 433: '25', 434: 'paul', 435: 'lucas', 436: 'brings', 437: 'tear', 438: 'davis', 439: 'sympathetic', 440: 'role', 441: 'delight', 442: 'grand', 443: 'dressed', 444: 'widget', 445: 'child', 446: 'fun', 447: 'mother', 448: 'slow', 449: 'awakening', 450: 'whats', 451: 'happening', 452: 'roof', 453: 'believable', 454: 'startling', 455: 'dozen', 456: 'thumb', 457: 'sure', 458: 'resurrection', 459: 'dated', 460: 'series', 461: 'tech', 462: 'bring', 463: 'back', 464: 'excitement', 465: 'mei', 466: 'grew', 467: 'black', 468: 'white', 469: 'gunsmoke', 470: 'hero', 471: 'weekyou', 472: 'vote', 473: 'comeback', 474: 'hunt', 475: 'need', 476: 'change', 477: 'pace', 478: 'water', 479: 'adventure', 480: 'thank', 481: 'outlet', 482: 'view', 483: 'viewpoint', 484: 'ole', 485: 'wan', 486: 'na', 487: 'saywould', 488: 'nice', 489: 'read', 490: 'plus', 491: 'rhyme', 492: 'line', 493: 'submit', 494: 'leave', 495: 'doubt', 496: 'quite', 497: 'amazing', 498: 'fresh', 499: 'innovative', 500: 'idea', 501: '70', 502: 'aired', 503: '7', 504: '8', 505: 'brilliant', 506: 'dropped', 507: '1990', 508: 'anymore', 509: 'continued', 510: 'decline', 511: 'complete', 512: 'waste', 513: 'disgraceful', 514: 'fallen', 515: 'writing', 516: 'painfully', 517: 'mildly', 518: 'entertaining', 519: 'respite', 520: 'guesthosts', 521: 'hard', 522: 'creator', 523: 'handselected', 524: 'also', 525: 'chose', 526: 'band', 527: 'hack', 528: 'followed', 529: 'recognize', 530: 'brilliance', 531: 'fit', 532: 'replace', 533: 'mediocrity', 534: 'felt', 535: 'star', 536: 'respect', 537: 'made', 538: 'huge', 539: 'cant', 540: 'encouraged', 541: 'positive', 542: 'comment', 543: 'looking', 544: 'forward', 545: 'mistake', 546: '950', 547: 'worst', 548: 'pacing', 549: 'storyline', 550: 'soundtrack', 551: 'song', 552: 'lame', 553: 'country', 554: 'tune', 555: 'played', 556: 'less', 557: 'four', 558: 'cheap', 559: 'extreme', 560: 'rarely', 561: 'happy', 562: 'end', 563: 'credit', 564: 'prevents', 565: 'giving', 566: '1score', 567: 'harvey', 568: 'least', 569: 'making', 570: 'bit', 571: 'effort', 572: 'obsessive', 573: 'gut', 574: 'reaching', 575: 'laughter', 576: 'hell', 577: 'mom', 578: 'liked', 579: 'camp', 580: 'phil', 581: 'alien', 582: 'quirky', 583: 'humour', 584: 'based', 585: 'everything', 586: 'actual', 587: 'punchlinesat', 588: 'odd', 589: 'progressed', 590: 'didnt', 591: 'joke', 592: 'anymoreits', 593: 'low', 594: 'budget', 595: 'thats', 596: 'problem', 597: 'eventually', 598: 'lost', 599: 'interest', 600: 'imagine', 601: 'stone', 602: 'currently', 603: 'partakingfor', 604: 'something', 605: 'try', 606: 'brother', 607: 'planet', 0: '<PAD>'}\n"
     ]
    }
   ],
   "source": [
    "print(int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 140, 141, 10, 142, 11, 62, 143, 26, 63, 144, 145, 7, 12, 64]\n",
      "[86, 34, 87, 232, 88, 233, 234, 235, 89, 236, 237, 238, 239, 90, 240]\n",
      "[100, 86, 13, 278, 22, 14, 279, 280, 281, 282, 37, 283, 284, 10, 285]\n",
      "[332, 105, 333, 34, 334, 24, 335, 105, 106, 107, 44, 336, 337, 4, 338]\n",
      "[48, 25, 111, 22, 14, 112, 363, 364, 2, 113, 49, 25, 365, 41, 366]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# encode words\n",
    "reviews_enc = [[word2int[word] for word in review.split()] for review in tqdm(reviews)]\n",
    "\n",
    "# print first-10 words of first 5 reviews\n",
    "for i in range(5):\n",
    "    print(reviews_enc[i][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1, 140, 141,  10, 142,  11,  62, 143,  26,  63],\n",
       "       [ 86,  34,  87, 232,  88, 233, 234, 235,  89, 236],\n",
       "       [100,  86,  13, 278,  22,  14, 279, 280, 281, 282],\n",
       "       [332, 105, 333,  34, 334,  24, 335, 105, 106, 107],\n",
       "       [ 48,  25, 111,  22,  14, 112, 363, 364,   2, 113],\n",
       "       [126, 421, 422,   4, 423, 424, 425, 426, 427, 428],\n",
       "       [457,   5,   9,   6, 458, 459, 132, 460, 461, 133],\n",
       "       [  3, 497, 498, 499, 500, 501,   7, 502,   7, 503],\n",
       "       [540, 541, 542,   2, 543, 544,  10,   2,  60, 545],\n",
       "       [  9,  61, 573, 574, 575,   9,   4, 103, 128, 111]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding sequences\n",
    "\n",
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    # features = np.zeros((len(reviews), seq_length), dtype=int)\n",
    "    features = np.full((len(reviews), seq_length), pad_id, dtype=int)      #Return a new array of given shape (len(reviews), seq_length) and type = int, filled with pad_id.\n",
    "\n",
    "    for i, row in enumerate(reviews):\n",
    "        # if seq_length < len(row) then review will be trimmed\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features\n",
    "\n",
    "seq_length = 256\n",
    "features = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)\n",
    "\n",
    "assert len(features) == len(reviews_enc)\n",
    "assert len(features[0]) == seq_length\n",
    "\n",
    "features[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labels as numpy\n",
    "labels = df_new.label.to_numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the corpus and target variables\n",
    "x = df.review\n",
    "y = df.label                                                                            \n",
    "\n",
    "# train test splitting\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.0005, random_state=0)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_sent_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
